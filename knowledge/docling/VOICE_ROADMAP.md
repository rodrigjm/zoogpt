# Voice Feature Project Plan: Zoocari Voice Mode

## Current Status: Kokoro Local TTS Integration

| Phase | Status | Description |
|-------|--------|-------------|
| Phase 1 | ‚úÖ Complete | Audio Input Integration |
| Phase 2 | ‚úÖ Complete | Text-to-Speech Output (ElevenLabs) |
| Phase 2.5 | üü° In Progress | **Kokoro Local TTS** (latency optimization) |
| Phase 3 | ‚úÖ Complete | UI/UX Enhancements |
| Phase 4 | ‚è≥ Pending | Integration & Polish |
| Phase 5 | ‚è≥ Pending | HTTPS Deployment (Mobile Support) |

---

## Architecture Overview: Chained Voice Pipeline

```
User Audio ‚Üí [Whisper STT] ‚Üí Text ‚Üí [GPT-4o-mini] ‚Üí Response ‚Üí [TTS] ‚Üí Audio
     ‚Üì            ‚Üì                      ‚Üì                       ‚Üì
st.audio_input  OpenAI API      Existing chat logic      Tiered Fallback:
                                                         1. Kokoro (local)
                                                         2. ElevenLabs (cloud)
                                                         3. OpenAI (cloud)
```

**Why chained architecture:**
- High control and transparency (full transcript available)
- Robust function calling support
- Reliable, predictable responses
- Ideal for structured workflows like kid-friendly Q&A

---

## Phase 2.5: Kokoro Local TTS üü° IN PROGRESS

**Goal**: Replace cloud TTS with local inference for reduced latency

### Latency Comparison
| Provider | Latency | Cost | Network |
|----------|---------|------|---------|
| **Kokoro (local)** | 50-200ms | Free | No |
| ElevenLabs | 500-2000ms | ~$0.30/1K chars | Yes |
| OpenAI TTS | 300-800ms | ~$0.015/1K chars | Yes |

### Implementation Tasks
| Task | Status | Description |
|------|--------|-------------|
| 2.5.1 | ‚úÖ | Add `kokoro`, `soundfile` to requirements.txt |
| 2.5.2 | ‚úÖ | Create `tts_kokoro.py` module |
| 2.5.3 | ‚úÖ | Implement tiered fallback in `generate_speech()` |
| 2.5.4 | ‚è≥ | Install espeak-ng system dependency |
| 2.5.5 | ‚è≥ | Test local inference |
| 2.5.6 | ‚è≥ | Add voice selection UI |

### Kokoro Voice Options (Kid-Friendly)
| Voice | ID | Description |
|-------|-----|-------------|
| Bella | `af_bella` | Friendly female (default) |
| Nova | `af_nova` | Warm, engaging female |
| Heart | `af_heart` | Expressive, upbeat female |
| Adam | `am_adam` | Clear male voice |

### System Requirements
```bash
# macOS
brew install espeak-ng

# Ubuntu/Debian
apt-get install espeak-ng
```

### New Files
- `tts_kokoro.py` - Local Kokoro TTS inference module

---

## Phased Project Roadmap

---

### Phase 1: Audio Input Integration ‚úÖ COMPLETE
**Goal**: Enable voice recording from the user

| Task | Status | Description |
|------|--------|-------------|
| 1.1 | ‚úÖ | Add `st.audio_input` widget for voice recording |
| 1.2 | ‚úÖ | Create `transcribe_audio()` function using Whisper |
| 1.3 | ‚úÖ | Add voice mode toggle in left panel |
| 1.4 | ‚úÖ | Handle audio file format (wav via BytesIO) |
| 1.5 | ‚úÖ | Update session state for voice input mode |

**Implementation**:
```python
def transcribe_audio(audio_bytes: bytes) -> str:
    """Convert audio to text using OpenAI's Whisper API."""
    audio_file = io.BytesIO(audio_bytes)
    audio_file.name = "recording.wav"
    transcription = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language="en",
    )
    return transcription.text
```

---

### Phase 2: Text-to-Speech Output ‚úÖ COMPLETE
**Goal**: Generate audio responses alongside text streaming

| Task | Status | Description |
|------|--------|-------------|
| 2.1 | ‚úÖ | Create `generate_speech()` with ElevenLabs API |
| 2.2 | ‚úÖ | Generate audio after text response completes |
| 2.3 | ‚úÖ | Display audio player using `st.audio()` |
| 2.4 | ‚úÖ | ElevenLabs voices (Rachel default) with OpenAI fallback |
| 2.5 | ‚úÖ | Store audio in session state for replay |

**Implementation** (ElevenLabs with OpenAI fallback):
```python
def generate_speech(text: str, voice_id: str = "21m00Tcm4TlvDq8ikWAM") -> bytes:
    """Convert text to speech using ElevenLabs (with OpenAI fallback)."""
    clean_text = # ... remove markdown ...

    if elevenlabs_client:
        audio = elevenlabs_client.text_to_speech.convert(
            voice_id=voice_id,
            text=clean_text[:5000],
            model_id="eleven_multilingual_v2",
            output_format="mp3_44100_128"
        )
        return b"".join(audio)

    # Fallback to OpenAI TTS
    with client.audio.speech.with_streaming_response.create(...) as response:
        return response.read()
```

**ElevenLabs Voice Options**:
| Voice | ID | Description |
|-------|-----|-------------|
| Rachel | `21m00Tcm4TlvDq8ikWAM` | Warm, friendly (default) |
| Bella | `EXAVITQu4vr4xnSDxMaL` | Young, energetic |
| Josh | `TxGEqnHWrfWFTfGW9XjX` | Friendly male |

---

### Phase 3: UI/UX Enhancements ‚úÖ COMPLETE
**Goal**: Seamless voice interaction experience

| Task | Status | Description |
|------|--------|-------------|
| 3.1 | ‚úÖ | Voice mode toggle button |
| 3.2 | ‚úÖ | Visual recording indicator (pulse animation) |
| 3.3 | ‚úÖ | Style audio player to match Leesburg branding |
| 3.4 | ‚úÖ | Call-to-action voice button design |
| 3.5 | ‚ùå | Mobile voice recording (requires HTTPS) |
| 3.6 | ‚úÖ | Auto-play TTS in voice mode |

**Blocker**: Mobile voice recording requires HTTPS (see Phase 5)

**Implementation Details**:
- Pulse animation on recording indicator with orange glow effect
- Voice mode status badge shows "Voice Active" / "Text Mode"
- Styled audio player with Leesburg brown/yellow gradient
- Voice panel with clear CTA and recording hints

---

### Phase 4: Integration & Polish ‚è≥ PENDING
**Goal**: Production-ready voice features

| Task | Status | Description |
|------|--------|-------------|
| 4.1 | ‚è≥ | Error handling for audio recording failures |
| 4.2 | ‚è≥ | Loading states during transcription/TTS |
| 4.3 | ‚úÖ | Fallback to text when voice unavailable |
| 4.4 | ‚è≥ | Voice-friendly response formatting |
| 4.5 | ‚è≥ | Voice selection UI for users |
| 4.6 | ‚úÖ | Update requirements.txt (elevenlabs added) |

---

### Phase 5: HTTPS Deployment ‚è≥ PENDING (NEW)
**Goal**: Enable mobile voice recording via secure context

| Task | Status | Description |
|------|--------|-------------|
| 5.1 | ‚è≥ | Choose deployment method |
| 5.2 | ‚è≥ | Configure HTTPS/SSL |
| 5.3 | ‚è≥ | Test mobile voice recording |
| 5.4 | ‚è≥ | Production deployment |

**Deployment Options**:

| Option | Complexity | Cost | Best For |
|--------|------------|------|----------|
| **Streamlit Cloud** | Easy | Free | Quick deployment |
| **Cloudflare Tunnel** | Easy | Free | Self-hosted, no ports |
| **ngrok** | Easy | Free/Paid | Testing only |
| **Nginx + Let's Encrypt** | Medium | Free | Production self-hosted |
| **Cloud Provider** | Medium | Varies | Scalable production |

**Docker + Nginx Setup** (for self-hosted):
```yaml
# docker-compose.yml additions
services:
  nginx:
    image: nginx:alpine
    ports:
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./certs:/etc/letsencrypt
    depends_on:
      - zoocari
```

---

## File Changes Summary

| File | Changes | Status |
|------|---------|--------|
| `zoo_chat.py` | STT function, TTS function (ElevenLabs), voice mode UI | ‚úÖ Done |
| `requirements.txt` | Added `elevenlabs` package | ‚úÖ Done |
| `.env` | `ELEVENLABS_API_KEY` (optional) | ‚úÖ Documented |
| `docker-compose.yml` | HTTPS proxy (Phase 5) | ‚è≥ Pending |
| `nginx.conf` | SSL configuration (Phase 5) | ‚è≥ Pending |

---

## Technical Decisions Made

| Decision | Choice | Rationale |
|----------|--------|-----------|
| STT Model | `whisper-1` | Stable, reliable for kids' speech |
| TTS Provider | ElevenLabs (primary) | More natural, expressive voices |
| TTS Fallback | OpenAI `tts-1` | Works without ElevenLabs key |
| Default Voice | Rachel | Warm, friendly for kids |
| Auto-play | Yes (voice mode only) | Natural conversation flow |

---

## Environment Variables

```bash
# Required
OPENAI_API_KEY=sk-...

# Optional (enables ElevenLabs TTS)
ELEVENLABS_API_KEY=...
```

---

## References

- [ElevenLabs Python SDK](https://github.com/elevenlabs/elevenlabs-python)
- [ElevenLabs Voice Library](https://elevenlabs.io/voice-library)
- [OpenAI Whisper API](https://platform.openai.com/docs/guides/speech-to-text)
- [OpenAI TTS API](https://platform.openai.com/docs/guides/text-to-speech)
- [Streamlit st.audio_input](https://docs.streamlit.io/develop/api-reference/widgets/st.audio_input)
- [MediaRecorder HTTPS Requirement](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia#privacy_and_security)
